from crewai import Agent, Task, Crew, Process
from typing import List
from loguru import logger 
from crewai.knowledge.source.string_knowledge_source import StringKnowledgeSource
from .config import (
    CRITIQUE_AGENT_PROMPT,
    CRITIQUE_AGENT_TASK,
    DYNAMIC_AGENT_TASK,
    META_AGENT_PROMPT,
    META_AGENT_TASK,
    MARKDOWN_TASK,
    MODEL,
    ALLOW_DELEGATION,
    LOG_PATH
)
from dotenv import load_dotenv
import openai
import os
openai.api_key = os.getenv("OPENAI_API_KEY")
  
load_dotenv()

def reflection(
    query:str,
    context:str,
    agents:List[Agent],
    agents_task:List[Task],
    meta_agent:Agent,
    meta_agent_task:Task,
    n:int=2
)-> str:
    
    string_source = StringKnowledgeSource(
        content=context,
        metadata={"preference": "personal"}
    )
    
    initial_crew = Crew(
        agents=[*agents,
                meta_agent
                ],

        tasks=[*agents_task,
               meta_agent_task
               ],

        process=Process.sequential,
        verbose=True,
        knowledge={"correct_context": [string_source], "metadata": {"preference": "personal"}},
        memory=False,
        output_log_file=LOG_PATH
    )

    inputs = {
        'que': query,
        'con': context
    }

    results_inital = initial_crew.kickoff(inputs=inputs)

    logger.info(f"initial crew response in reflection-- {results_inital}")

    critique_agent= Agent(
        role= CRITIQUE_AGENT_PROMPT["role"],
        goal= CRITIQUE_AGENT_PROMPT["goal"],
        backstory= CRITIQUE_AGENT_PROMPT["backstory"],
        verbose=True,
        allow_delegation= ALLOW_DELEGATION,
        llm = MODEL
    )

    critique_task_description = CRITIQUE_AGENT_TASK["description"].format(que=query, con=context)
    critique_task = Task(
        description=critique_task_description,
        expected_output=CRITIQUE_AGENT_TASK["expected_output"],
        agent=critique_agent,
        context=[meta_agent_task],
        async_execution=False,
        output_log_file=LOG_PATH
    )

    dynamic_task_list = []
    dynamic_task_description = DYNAMIC_AGENT_TASK["description"].format(que=query, con=context)
    for i in range(len(agents)):
        dynamic_agent_task = Task(
            description=dynamic_task_description,
            expected_output=DYNAMIC_AGENT_TASK["expected_output"],
            agent=agents[i],
            context=[critique_task],
            async_execution=True,
        )
        dynamic_task_list.append(dynamic_agent_task)


    final_agent= Agent(
            role= META_AGENT_PROMPT["role"],
            goal= META_AGENT_PROMPT["goal"],
            backstory= META_AGENT_PROMPT["backstory"],
            verbose=True,
            allow_delegation= ALLOW_DELEGATION,
            llm = MODEL
        )
    
    meta_agent_task_description = META_AGENT_TASK["description"].format(que=query)
    final_agent_task = Task(
            description=meta_agent_task_description,
            expected_output=META_AGENT_TASK["expected_output"],
            agent=final_agent,
            context= dynamic_task_list,
            async_execution=False
        )
    
    logger.info(f"new tasks and final agent formed---")

    reflection_crew = Crew(
        agents=[critique_agent,
                *agents,
                final_agent],
        
        tasks=[critique_task,
               dynamic_agent_task,
               final_agent_task],
        
        process=Process.sequential,
        verbose=True,
        knowledge={"correct_context": [string_source], "metadata": {"preference": "personal"}},
        memory=False,
        output_log_file=LOG_PATH
    )

    logger.info(f"reflection crew formed---")

    inputs = {
        'question': query,
        'responses': results_inital
    }
    ans=reflection_crew.kickoff(inputs=inputs)

    inputs['responses'] = ans

    logger.info(f"reflection at n=1--{inputs['responses']}")

    critique_task.context = [final_agent_task]
    for i in range(len(dynamic_task_list)):
        dynamic_task_list[i].context = [critique_task]
        final_agent_task.context = dynamic_task_list

    for _ in range(n-1):
        if _ == n-2:
            final_agent_task.expected_output = MARKDOWN_TASK
        ans = reflection_crew.kickoff(inputs=inputs)
        inputs['responses'] = ans
        logger.info(f"reflection at n={_+2}--{inputs['responses']}")

    logger.info(f"here is the final output from reflection crew:---{inputs['responses']}")
    return inputs['responses']
